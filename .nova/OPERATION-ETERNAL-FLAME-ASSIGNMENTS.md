# OPERATION ETERNAL FLAME — Complete Assignment Package
## February 20, 2026 — 24-Hour Sprint

> 8 coding workers × 10+ tasks each = 80+ implementation tasks
> 4 browser agents × 10 series = 40 research/review prompts
> Deliver one series at a time: Grok → Gemini → ChatGPT → Perplexity

---

# PART 1: CODING AGENT SPRINTS (Terminal Workers)

Each agent gets 10+ detailed tasks spanning all 3 waves (0-8h, 8-16h, 16-24h).
Paste the UNIFIED CONTEXT block first, then the agent-specific sprint below it.

---

## SPRINT: SONNET 4.6 — "The Architect"

**Provider**: Anthropic (terminal / Claude Code)
**Domain**: `next.config.*`, `tailwind.config.*`, `app/layout.tsx`, `app/providers.tsx`, `app/(auth)/`, `src/orchestrator/`, `src/convex/`
**Cannot touch**: `src/mcp/`, `src/acp/`

### Task S1: Next.js 15 Project Configuration (Wave 1, 0-1h)
Create `next.config.ts` with full Next.js 15 App Router configuration. Enable `serverExternalPackages` for Convex. Configure `images.remotePatterns` for any CDN domains. Set `experimental.typedRoutes: true`. Create `postcss.config.js` with Tailwind CSS plugin. Update `tsconfig.json` to include `"moduleResolution": "bundler"`, `"target": "ES2022"`, path aliases (`@/` → `./`, `@convex/` → `./convex/_generated/`). Verify the project compiles with `npm run build` — zero errors. If `package.json` is missing any deps (`next`, `react`, `react-dom`, `convex`, `@auth/core`), add them. This is the foundation — nothing else works until this is clean.

### Task S2: Tailwind CSS + shadcn/ui Setup (Wave 1, 1-2h)
Create or update `tailwind.config.ts` with the shadcn/ui preset. Configure `content` paths to scan `app/**/*.{ts,tsx}` and `src/**/*.{ts,tsx}`. Set up the Nova26 design tokens: primary color `hsl(222.2, 84%, 4.9%)`, accent `hsl(210, 40%, 96.1%)`, destructive `hsl(0, 84.2%, 60.2%)`. Add `darkMode: "class"` support. Create `app/globals.css` with Tailwind directives (`@tailwind base; @tailwind components; @tailwind utilities;`) plus CSS custom properties for shadcn theming. Install shadcn/ui CLI and init: `npx shadcn-ui@latest init`. Add core components: Button, Card, Badge, Table, Tabs, Skeleton, Dialog, DropdownMenu, Input, Label, Separator, Sheet, Tooltip. Verify all imports resolve.

### Task S3: ConvexProvider + App Layout Wiring (Wave 1, 2-3h)
Create `app/providers.tsx` — a client component that wraps the entire app with `ConvexProviderWithAuth`. Import `ConvexReactClient` from `convex/react`. Initialize with `process.env.NEXT_PUBLIC_CONVEX_URL`. Wrap children in `<ConvexProvider client={convex}>`. Add `ThemeProvider` from `next-themes` for dark mode. Add error boundary at the provider level. Update `app/layout.tsx` to import and use `<Providers>{children}</Providers>`. Set metadata: title "Nova26 — AI-Powered IDE", description, viewport. Add Inter font from `next/font/google`. Ensure the layout renders server-side with client providers hydrating correctly. Test: app loads without hydration errors.

### Task S4: Convex Auth Integration — Core (Wave 1, 3-4.5h)
Implement authentication using Convex Auth (NOT Clerk, NOT NextAuth — Convex's built-in auth). Create `convex/auth.config.ts` with password provider and optional GitHub OAuth. Create `convex/auth.ts` using `convexAuth()` from `@convex-dev/auth/server`. Export `{ auth, signIn, signOut, store }`. Create `src/convex/auth-helpers.ts` with `requireAuth(ctx)` helper that throws `ConvexError("Not authenticated")` if no identity. Create `app/(auth)/layout.tsx` — centered card layout for auth pages. Create `app/(auth)/sign-in/page.tsx` — email + password form using shadcn Input/Button/Card, calls `signIn("password", { email, password })`. Create `app/(auth)/sign-up/page.tsx` — registration form with name, email, password, confirm password. Add form validation (email format, password min 8 chars, passwords match). Wire auth state into providers — `useConvexAuth()` hook for checking `isAuthenticated`, `isLoading`. Add redirect: authenticated users hitting `/sign-in` → `/dashboard`.

### Task S5: Auth Middleware + Protected Routes (Wave 1, 4.5-6h)
Create `middleware.ts` at project root. Protect all `/dashboard/*` routes — redirect unauthenticated users to `/sign-in`. Allow public access to `/`, `/sign-in`, `/sign-up`, and all static assets. Store the intended destination in a query param (`?redirect=/dashboard/builds`) so after sign-in the user lands where they wanted. Create `app/(auth)/components/auth-guard.tsx` — client component that checks `useConvexAuth()` and shows a loading spinner while auth state resolves, redirects to sign-in if not authenticated. Create `app/(auth)/components/user-button.tsx` — dropdown showing user email, avatar placeholder, sign-out button. Wire `user-button.tsx` into the dashboard layout header. Test the full flow: unauthenticated → `/sign-in` → sign up → redirected to `/dashboard` → see user button → sign out → back to landing.

### Task S6: Engine → Convex Bridge (Wave 1, 6-8h)
Create `src/convex/bridge.ts` — the critical link between the ralph-loop engine (which runs locally) and Convex (which persists data). Import `ConvexHttpClient` from `convex/browser`. Create `ConvexBridge` class with methods: `logBuild(build: BuildResult)` → calls `api.dashboard.createBuild` mutation, `logTask(task: TaskResult)` → calls `api.dashboard.createTask` mutation, `logExecution(exec: ExecutionLog)` → calls `api.dashboard.logExecution` mutation, `logActivity(event: ActivityEvent)` → calls `api.realtime.logActivity` mutation, `syncAgentStatus(agentId: string, status: AgentStatus)` → calls `api.dashboard.updateAgentStatus` mutation. Each method should: validate input with Zod schemas, handle Convex errors gracefully (retry once on network failure, log on validation failure), batch writes when possible (use `Promise.all` for independent mutations). Create `src/convex/bridge.test.ts` with 15+ tests: mock ConvexHttpClient, verify correct mutations called, verify error handling, verify retry logic, verify Zod validation rejects bad input. Wire the bridge into `src/orchestrator/ralph-loop.ts` — after each phase completion, call `bridge.logActivity()`. After build completion, call `bridge.logBuild()`.

### Task S7: Integration Testing — Auth + Dashboard + Convex (Wave 2, 8-10h)
Write integration tests that verify the full auth → dashboard → data flow. Create `tests/integration/auth-flow.test.ts`: test sign-up creates user in Convex, test sign-in returns valid session, test protected route redirects without auth, test auth state persists across page navigation. Create `tests/integration/dashboard-data.test.ts`: test dashboard overview loads agent stats, test builds page shows build history, test activity feed receives real-time updates. Create `tests/integration/bridge-e2e.test.ts`: test engine build completion → Convex mutation → dashboard query returns new build. Use Convex test utilities (`convex-test`) for isolated test environments. Mock auth for dashboard tests. Verify 0 TypeScript errors across all test files. Target: 25+ integration tests covering the critical path.

### Task S8: Error Handling + Edge Cases (Wave 2, 10-12h)
Audit all auth and bridge code for error handling gaps. Add: Convex connection failure handling in providers (show "Connecting..." state, retry with exponential backoff). Auth token expiry handling — detect 401 from Convex, redirect to sign-in with message "Session expired". Bridge write failure handling — queue failed writes in memory, retry on next successful connection. Rate limiting on auth endpoints — max 5 sign-in attempts per minute per IP (implement in Convex mutation with a `rateLimits` table). Add proper error types: `AuthError`, `BridgeError`, `ConnectionError` — all extending a base `Nova26Error`. Create `src/convex/error-types.ts`. Update bridge tests to cover: network timeout, invalid auth token, malformed data, rate limit exceeded. Target: 15+ new error handling tests.

### Task S9: Performance Optimization + Caching (Wave 2, 12-14h)
Optimize the Convex provider setup for minimal re-renders. Implement `React.memo` on heavy dashboard components. Add `useMemo` for computed values (agent stats aggregation, build duration calculations). Optimize Convex queries: ensure all queries use indexed fields (check against `convex/schema.ts` indexes), add `.paginate()` for builds list (default 20 per page), add cursor-based pagination for activity feed. Create `src/convex/query-helpers.ts` with: `paginatedQuery(table, index, cursor, limit)` helper, `cachedQuery` wrapper that deduplicates identical in-flight queries. Benchmark: dashboard overview should load in <500ms with 100 builds in the database. Add `console.time`/`console.timeEnd` instrumentation in development mode only. Write 10+ tests for pagination and caching helpers.

### Task S10: Final Integration + Smoke Testing (Wave 3, 16-20h)
Run the complete application end-to-end. Verify: landing page loads → click "Get Started" → sign-up page → create account → redirected to dashboard → overview shows agent stats → navigate to builds → see build history → navigate to agents → see 21 agent cards → navigate to settings → see user profile → sign out → back to landing. Fix any broken links, missing data, or UI glitches found during smoke testing. Verify all Convex subscriptions update in real-time (open two browser tabs, trigger activity in one, see it appear in the other). Run `npx tsc --noEmit` — must be 0 errors. Run `vitest run` — all tests must pass. Document any remaining issues as GitHub issues with clear reproduction steps. Create a `SMOKE-TEST-RESULTS.md` in `.nova/` with pass/fail for each flow.

### Task S11: Production Hardening (Wave 3, 20-24h)
Final production readiness pass. Remove all `console.log` statements (replace with structured logging if needed). Ensure all environment variables are documented in `.env.local.example`. Add `NEXT_PUBLIC_` prefix only to client-safe vars. Verify CSP headers in `next.config.ts` — allow Convex domain, block inline scripts. Add `robots.txt` and `sitemap.xml` for SEO. Verify `<meta>` tags on all pages (title, description, og:image). Test with JavaScript disabled — ensure graceful degradation message. Run Lighthouse audit — target 90+ on Performance, Accessibility, Best Practices. Fix any critical Lighthouse findings. Final `tsc --noEmit` + `vitest run` — zero errors, zero failures.

---

## SPRINT: HAIKU 4 — "The Backend Glue"

**Provider**: Anthropic (terminal)
**Domain**: `convex/*.ts` (NOT schema.ts), `vercel.json`, `.env*`, `convex.json`
**Cannot touch**: `src/`, `app/(landing)/`

### Task H1: Core Dashboard Queries (Wave 1, 0-2h)
Create `convex/dashboard.ts`. Implement `listBuilds` query: paginated (default 20), sorted by `startedAt` desc, uses `by_company` index, returns `{ builds, nextCursor, hasMore }`. Auth required via `ctx.auth.getUserIdentity()`. Implement `getBuild` query: takes `buildId: v.id("builds")`, returns build with its tasks joined via `by_build` index on tasks table. Implement `listTasks` query: takes `buildId`, returns tasks sorted by phase number then sequence. Implement `getOverviewStats` query: returns `{ totalBuilds, activeTasks, successRate, lastBuildTime, totalAgents }` — aggregate from builds and tasks tables using appropriate indexes. All functions must use Convex validators (`v.string()`, `v.number()`, `v.id()`, `v.optional()`). All functions must check auth. All return types must be explicit. Write 20+ unit tests in `convex/dashboard.test.ts` using `convex-test`.

### Task H2: Agent Stats + Activity Functions (Wave 1, 2-4h)
Add to `convex/dashboard.ts`: `getAgentStats` query — for each of the 21 agents, return `{ agentId, name, role, totalTasks, successRate, avgDuration, lastActive, currentStatus }`. Use the `agents` table joined with `tasks` via `by_agent` index. Cache-friendly: this query should be efficient even with 10K+ tasks. Create `convex/realtime.ts`. Implement `subscribeToActivity` query: returns latest 50 activity feed items sorted by `timestamp` desc, uses `by_timestamp` index on `agentActivityFeed` table. Implement `subscribeToBuilds` query: returns all builds with status "running" or "pending", sorted by `startedAt` desc. Implement `logActivity` mutation: takes `{ agentName, action, details, status }`, writes to `agentActivityFeed` with server timestamp. Validate all inputs. Write 15+ tests.

### Task H3: Auth + User Functions (Wave 1, 4-5.5h)
Create `convex/auth.ts`. Implement `getCurrentUser` query: get user identity from `ctx.auth.getUserIdentity()`, look up in `userProfiles` table by `tokenIdentifier`, return profile or null. Implement `ensureUser` mutation: called on first login — check if user exists in `userProfiles`, if not create with defaults `{ name, email, tier: "free", settings: { theme: "system", notifications: true } }`. Return the user profile. Create `convex/users.ts`. Implement `getUser` query: by `userId: v.id("userProfiles")`. Implement `updateSettings` mutation: takes partial settings object, merges with existing, validates each field. Implement `updateTier` mutation: takes `tier: v.union(v.literal("free"), v.literal("pro"), v.literal("team"), v.literal("enterprise"))`, updates user tier. All mutations require auth. Write 15+ tests covering: new user creation, duplicate prevention, settings merge, tier validation, unauthorized access rejection.

### Task H4: Build + Task Mutation Functions (Wave 1, 5.5-7h)
Add to `convex/dashboard.ts`: `createBuild` mutation — takes `{ name, config, agentIds }`, creates build record with status "pending", startedAt = server time, returns buildId. `updateBuildStatus` mutation — takes `{ buildId, status, error? }`, updates status, sets `completedAt` if status is "completed" or "failed". `createTask` mutation — takes `{ buildId, agentId, phase, description }`, creates task linked to build. `updateTaskStatus` mutation — takes `{ taskId, status, output?, error?, duration? }`, updates task. `logExecution` mutation — takes `{ taskId, agentId, input, output, duration, tokensUsed }`, writes to executions table. `updateAgentStatus` mutation — takes `{ agentId, status, currentTask? }`, updates agent record. All mutations validate inputs, require auth, and log activity via internal `logActivity` call. Write 20+ tests covering: build lifecycle (pending → running → completed), task lifecycle, execution logging, agent status transitions, validation failures.

### Task H5: Deployment Configuration (Wave 1, 7-8h)
Create `vercel.json`: configure build command `npx convex deploy && next build`, output directory `.next`, framework `nextjs`. Set headers: `X-Frame-Options: DENY`, `X-Content-Type-Options: nosniff`, `Strict-Transport-Security: max-age=31536000`. Configure rewrites if needed for Convex. Create `.env.local.example` documenting ALL required env vars: `NEXT_PUBLIC_CONVEX_URL`, `CONVEX_DEPLOY_KEY`, `AUTH_SECRET` (for Convex Auth), `GITHUB_CLIENT_ID` + `GITHUB_CLIENT_SECRET` (optional OAuth). Create `convex.json` if not exists — configure project name, team. Verify `npx convex dev` connects and syncs schema without errors. Verify `npx convex deploy --dry-run` succeeds. Document the deployment steps in comments within `vercel.json`.

### Task H6: Index Optimization + Query Performance (Wave 2, 8-10h)
Audit ALL queries in `convex/dashboard.ts`, `convex/realtime.ts`, `convex/auth.ts`, `convex/users.ts`. For each query, verify it uses an index (no full table scans). Cross-reference with `convex/schema.ts` — if a query needs an index that doesn't exist, document it as a blocker for schema update. Optimize `getAgentStats` — this is the most expensive query. Consider: pre-computing stats in a `agentStatsCache` table updated by a Convex cron job, or computing incrementally on each task completion. Implement whichever approach is cleaner. Add `.paginate()` to all list queries that could return 100+ results. Add `withIndex()` explicitly to all queries (don't rely on implicit index selection). Benchmark: run each query with 100, 1000, 10000 rows and log execution time. Write 10+ performance-focused tests.

### Task H7: Convex Cron Jobs + Background Tasks (Wave 2, 10-12h)
Create `convex/crons.ts`. Implement: `cleanupOldActivity` — runs every hour, deletes activity feed items older than 7 days. `computeAgentStats` — runs every 5 minutes, aggregates task data per agent and writes to a `agentStatsCache` table (create if needed). `checkStalledBuilds` — runs every 10 minutes, finds builds with status "running" that haven't updated in 30 minutes, marks them as "stalled". Register all crons using `cronJobs()` from `convex/server`. Create `convex/internal.ts` for internal-only functions that crons call (not exposed to client). Write 10+ tests: verify cron logic (mock time), verify cleanup removes old items, verify stats computation is accurate, verify stalled build detection.

### Task H8: Webhook + External Integration Support (Wave 2, 12-14h)
Create `convex/webhooks.ts`. Implement `handleGitHubWebhook` HTTP action — receives GitHub push/PR events, logs them as activity. Implement `handleBuildComplete` HTTP action — external CI can notify Nova26 when a build finishes. Implement `handleAlertWebhook` HTTP action — receives monitoring alerts, creates activity feed items with status "alert". All webhooks: validate signatures (HMAC-SHA256 for GitHub), rate limit (max 100/minute per source), log all incoming requests for debugging. Create `convex/http.ts` to register HTTP routes: `POST /webhooks/github`, `POST /webhooks/build`, `POST /webhooks/alert`. Write 15+ tests: valid webhook processing, signature validation failure, rate limiting, malformed payload handling.

### Task H9: Production Deployment (Wave 3, 16-18h)
Deploy Convex to production: `npx convex deploy`. Verify all tables created, all indexes built, all functions accessible. Deploy Next.js to Vercel: connect GitHub repo, set environment variables, trigger build. Verify production URLs: landing page loads, auth pages work, dashboard loads with real data. Run smoke test on production: sign up → sign in → dashboard → create test build via bridge → see it appear → sign out. Check Convex dashboard for: function execution logs, no errors, reasonable latency (<200ms for queries). Document any production-specific configuration in `.nova/DEPLOYMENT-LOG.md`.

### Task H10: Monitoring + Alerting Setup (Wave 3, 18-22h)
Create `convex/monitoring.ts`. Implement `getSystemHealth` query — returns: total function calls in last hour, error rate, average latency, active subscriptions count, database size estimate. Implement `getErrorLog` query — returns last 100 errors across all functions, grouped by function name. Implement `logError` internal mutation — called from catch blocks in all functions, writes to an `errorLog` table with: function name, error message, stack trace, timestamp, userId. Add try/catch with `logError` to ALL existing mutations (dashboard, auth, users, realtime, webhooks). Create a simple health check endpoint: `convex/http.ts` → `GET /health` returns `{ status: "ok", timestamp, version }`. Write 10+ tests for monitoring functions.

### Task H11: Data Migration + Seed Scripts (Wave 3, 22-24h)
Create `convex/seed.ts` — a one-time mutation that populates the database with demo data for the 21 agents. For each agent: create an entry in the `agents` table with name, role, model, status "idle". Create 5 sample builds with varying statuses (completed, failed, running). Create 20 sample tasks across those builds. Create 50 sample activity feed items spanning the last 24 hours. This seed data makes the dashboard look alive for demos. Add a `convex/migrations.ts` file with a migration framework: version tracking table, up/down functions, idempotent execution. Implement migration 001: ensure all existing records have required fields (backfill nulls with defaults). Write 10+ tests for seed data correctness and migration idempotency.

---

## SPRINT: KIMI 2.5 — "The Implementer"

**Provider**: Moonshot (swarm)
**Domain**: `src/integrations/`, `src/llm/`, `src/swarm/`
**Cannot touch**: `convex/`, `app/`, `src/orchestrator/ralph-loop.ts`


### Task K1: Perplexity Client — Core API Integration (Wave 1, 0-1.5h)
Create `src/integrations/perplexity-client.ts`. Implement `PerplexityClient` class with constructor taking `{ apiKey, baseUrl?, timeout? }`. Implement `search(query: string, options?: SearchOptions)` method — calls Perplexity's sonar API endpoint, returns `{ answer: string, citations: Citation[], confidence: number, model: string }`. `SearchOptions` includes: `{ maxTokens?: number, temperature?: number, searchDomain?: string[], returnCitations?: boolean, searchRecency?: "day" | "week" | "month" | "year" }`. Implement request/response Zod schemas for type safety. Handle API errors: rate limiting (429 → exponential backoff with jitter, max 3 retries), auth failure (401 → throw `PerplexityAuthError`), server error (5xx → retry once), timeout (→ throw `PerplexityTimeoutError`). Implement request deduplication — if the same query is in-flight, return the existing promise instead of making a duplicate call. Add structured logging for all API calls (request timestamp, duration, token count, status). Write 15+ tests mocking the HTTP layer.

### Task K2: Perplexity Client — Research Agent Integration (Wave 1, 1.5-3h)
Create `src/integrations/perplexity-research-agent.ts`. Implement `PerplexityResearchAgent` class that wraps `PerplexityClient` and provides higher-level research capabilities. Methods: `researchTopic(topic: string, depth: "shallow" | "medium" | "deep")` — shallow = 1 query, medium = 3 queries with follow-ups, deep = 5 queries with synthesis. `compareOptions(options: string[], criteria: string[])` — research each option against each criterion, return comparison matrix. `factCheck(claim: string)` — verify a claim with citations, return `{ verified: boolean, confidence: number, evidence: Citation[] }`. `summarizeFindings(queries: string[])` — batch research and synthesize into a structured summary. Each method should: validate inputs, handle partial failures gracefully (if 1 of 5 queries fails, return results for the other 4 with a warning), track token usage for cost monitoring. Wire into the agent system: register as a tool available to any agent via the tool registry in `src/orchestrator/`. Write 10+ tests covering multi-query flows, partial failures, and synthesis logic.

### Task K3: Perplexity Client — Caching + Cost Control (Wave 1, 3-4h)
Add caching layer to `PerplexityClient`. Create `src/integrations/perplexity-cache.ts`. Implement `PerplexityCache` with: in-memory LRU cache (max 1000 entries, configurable), TTL per entry (default 1 hour for search results, 24 hours for fact checks), cache key = SHA-256 hash of `query + JSON.stringify(options)`. Cache hit → return immediately without API call. Cache miss → call API, store result, return. Add `invalidate(query)` and `clear()` methods. Add cost tracking: `CostTracker` class that logs every API call with estimated token cost. Methods: `getUsage(period: "hour" | "day" | "month")` → returns `{ calls, inputTokens, outputTokens, estimatedCost }`. `setBudget(maxCostPerDay: number)` → throws `BudgetExceededError` if daily spend exceeds limit. `getRemainingBudget()` → returns dollars remaining today. Wire cost tracker into `PerplexityClient` — check budget before every API call. Write 10+ tests: cache hit/miss, TTL expiry, LRU eviction, budget enforcement, cost calculation accuracy.

### Task K4: Agent Model Routing — Core Router (Wave 1-2, 4-7h)
Spec: `.nova/specs/grok-r22-01-model-routing.md`. Create `src/llm/model-router.ts`. Implement `ModelRouter` class. Core concept: each of the 21 agents can be routed to different LLM models based on task type, cost constraints, and historical performance. Implement `RouteDecision route(agentId: string, taskType: TaskType, constraints: RoutingConstraints)` — returns `{ model: ModelConfig, reason: string, confidence: number, estimatedCost: number, estimatedLatency: number }`. `RoutingConstraints` includes: `{ maxCost?: number, maxLatency?: number, minQuality?: number, preferLocal?: boolean }`. Implement the UCB (Upper Confidence Bound) routing algorithm: maintain a `ModelStats` record per model with `{ totalCalls, totalReward, avgLatency, avgCost, successRate }`. UCB score = `avgReward + sqrt(2 * ln(totalCalls) / modelCalls)`. Select the model with highest UCB score that satisfies constraints. Implement `updateStats(model: string, result: TaskResult)` — update the running statistics after each task completion. Implement `getModelRanking(taskType: TaskType)` — return all models sorted by UCB score for a given task type. Create model registry: `src/llm/model-registry.ts` with configs for Ollama (local), OpenRouter models, Anthropic, Moonshot. Each config: `{ id, name, provider, costPerInputToken, costPerOutputToken, maxTokens, capabilities: string[], latencyP50, latencyP99 }`. Write 25+ tests: UCB convergence (after 100 simulated tasks, best model should be selected >60% of the time), constraint satisfaction, fallback when preferred model unavailable, stats update accuracy.

### Task K5: Agent Model Routing — Speculative Decoding (Wave 2, 7-9h)
Create `src/llm/speculative-decoder.ts`. Implement speculative decoding: use a fast/cheap model (Haiku, local Ollama) to generate a draft, then verify with a stronger model (Sonnet, GPT-4). `speculativeDecode(prompt: string, draftModel: ModelConfig, verifyModel: ModelConfig, options?: SpecOptions)` — returns `{ output: string, draftAcceptRate: number, totalLatency: number, costSaved: number }`. `SpecOptions`: `{ maxDraftTokens?: number, acceptanceThreshold?: number, parallelDrafts?: number }`. Algorithm: 1) Generate N draft tokens with fast model, 2) Send draft + prompt to verify model, 3) Verify model accepts/rejects each token, 4) Return accepted prefix + verify model's continuation. Track acceptance rate per model pair — if draft model consistently gets <30% acceptance, stop using speculative decoding for that pair. Implement `SpeculativeDecodingManager` that decides when to use speculative decoding vs direct generation based on: task complexity (simple tasks → direct with cheap model), latency budget (tight → direct), cost budget (tight → speculative with cheap draft). Write 20+ tests: acceptance rate tracking, model pair selection, fallback to direct generation, cost savings calculation.

### Task K6: Agent Model Routing — Agent-Specific Profiles (Wave 2, 9-11h)
Create `src/llm/agent-profiles.ts`. Define routing profiles for all 21 agents. Each profile: `{ agentId, preferredModels: ModelConfig[], taskTypeOverrides: Map<TaskType, ModelConfig>, costBudgetPerHour: number, qualityThreshold: number, latencyBudget: number }`. Example profiles: SUN (orchestrator) → needs high reasoning, prefer Sonnet/GPT-4, high cost budget. VENUS (frontend) → needs code generation, prefer Qwen Coder/DeepSeek, medium cost. SATURN (testing) → needs code + reasoning, prefer Sonnet, medium cost. HAIKU (backend) → needs Convex knowledge, prefer Haiku (fast + cheap), low cost. Implement `ProfileManager` class: `getProfile(agentId)`, `updateProfile(agentId, updates)`, `resetToDefaults(agentId)`. Implement `optimizeProfiles()` — analyze historical task results and suggest profile adjustments (e.g., "VENUS performs 15% better with DeepSeek than Qwen on component generation tasks"). Wire profiles into `ModelRouter.route()` — profile constraints are combined with per-request constraints (most restrictive wins). Write 15+ tests: profile loading, constraint merging, optimization suggestions, default fallbacks.

### Task K7: Agent Model Routing — Cost Optimization Engine (Wave 2, 11-13h)
Create `src/llm/cost-optimizer.ts`. Implement `CostOptimizer` class that manages the overall LLM spend across all agents. `setBudget(daily: number, hourly?: number, perAgent?: Map<string, number>)` — set spending limits. `canAfford(model: ModelConfig, estimatedTokens: number)` — check if a request fits within budget. `recordSpend(model: string, inputTokens: number, outputTokens: number)` — track actual spend. `getSpendReport(period: "hour" | "day" | "week")` — return `{ totalSpend, byModel: Map<string, number>, byAgent: Map<string, number>, projectedDaily, budgetRemaining }`. Implement smart downgrade logic: when budget is 80% consumed, automatically route to cheaper models. When 95% consumed, only allow critical tasks (SUN orchestration, MERCURY validation). When 100% consumed, queue non-critical tasks until next budget period. Implement `CostProjector` — based on current spend rate, project when budget will be exhausted. Alert at 50%, 75%, 90% thresholds. Wire into `ModelRouter` — cost optimizer is consulted before every routing decision. Write 15+ tests: budget enforcement, automatic downgrade, spend tracking accuracy, projection accuracy, threshold alerts.

### Task K8: Swarm Integration — Multi-Model Orchestration (Wave 2, 13-15h)
Update `src/swarm/` to use the new model router. Create `src/swarm/multi-model-swarm.ts`. Implement `MultiModelSwarm` class that can run multiple agents in parallel, each on their optimal model. `executeParallel(tasks: SwarmTask[])` — route each task to its optimal model via `ModelRouter`, execute all in parallel, collect results. Handle: partial failures (if 1 of 5 tasks fails, return results for the other 4), timeout per task, cost tracking across all parallel tasks. Implement `executeSequential(pipeline: SwarmPipeline)` — chain of tasks where output of one feeds into the next, potentially switching models between steps. Implement `executeFanOut(task: SwarmTask, models: ModelConfig[])` — run the same task on multiple models, return the best result (by quality score) or consensus result. Add circuit breaker per model: if a model fails 3 times in 5 minutes, temporarily remove it from routing for 10 minutes. Wire into the existing swarm system in `src/swarm/`. Write 15+ tests: parallel execution, sequential pipeline, fan-out consensus, circuit breaker activation/recovery.

### Task K9: Observability Integration (Wave 3, 15-18h)
Create `src/integrations/observability-bridge.ts`. Bridge between the model router and the observability eval framework (being built by Llama 4 Maverick in `src/observability/`). Log every routing decision: `{ timestamp, agentId, taskType, selectedModel, ucbScore, constraints, alternatives }`. Log every model call: `{ timestamp, model, inputTokens, outputTokens, latency, success, cost }`. Log every speculative decoding attempt: `{ draftModel, verifyModel, acceptanceRate, latencySaved, costSaved }`. Implement `RouterDashboard` data provider — aggregates routing data for visualization: model usage distribution (pie chart data), cost over time (line chart data), latency percentiles per model (bar chart data), UCB score evolution (line chart data). Create Zod schemas for all log entries. Write 10+ tests for log formatting, aggregation accuracy, and dashboard data generation.

### Task K10: End-to-End Integration + Hardening (Wave 3, 18-22h)
Integration test the complete flow: Agent receives task → ModelRouter selects model → SpeculativeDecoder decides strategy → LLM call executes → CostOptimizer records spend → ObservabilityBridge logs everything → Stats update for UCB. Create `src/llm/integration.test.ts` with 20+ tests covering: full routing flow with mocked LLM responses, budget exhaustion triggers downgrade, speculative decoding improves latency for simple tasks, UCB converges to best model after 50+ tasks, circuit breaker activates on repeated failures, cost projections are within 20% of actual spend. Harden all error paths: what happens if model registry is empty? If all models are circuit-broken? If budget is $0? If Perplexity API is down during research? Each edge case should have a graceful fallback and a test. Final pass: `tsc --noEmit` = 0 errors across all `src/integrations/` and `src/llm/` files.

---

## SPRINT: DEEPSEEK V3.2 — "The Frontend Builder"

**Provider**: OpenRouter (aider)
**Domain**: `app/(dashboard)/`, `app/components/sidebar.tsx`
**Cannot touch**: `src/`, `convex/*.ts` (functions)

### Task D1: Dashboard Layout Shell (Wave 1, 0-1.5h)
Create `app/(dashboard)/layout.tsx`. This is the main dashboard layout wrapping all dashboard pages. Structure: fixed sidebar (280px wide on desktop, collapsible on mobile) + header bar (64px tall, sticky) + main content area (scrollable). Sidebar slot: import `Sidebar` from `@/app/components/sidebar`. Header: show page title (dynamic based on route), search input (placeholder for now), notification bell icon, `UserButton` component. Main content: `{children}` with padding `p-6`. Add breadcrumb component below header showing current path. Use Tailwind classes exclusively. Dark mode support via `dark:` variants. Mobile: sidebar hidden by default, hamburger menu in header to toggle. Use shadcn `Sheet` component for mobile sidebar overlay. Wrap in auth guard — redirect to `/sign-in` if not authenticated.

### Task D2: Dashboard Overview Page (Wave 1, 1.5-3h)
Create `app/(dashboard)/page.tsx`. This is the main dashboard landing page. Layout: 4 stat cards at top (grid, 2×2 on mobile, 4×1 on desktop) + agent status section + recent activity section. Stat cards using shadcn `Card`: "Total Builds" (number + trend arrow), "Active Tasks" (number + "running now" label), "Success Rate" (percentage + color-coded), "Last Build" (relative time like "2 min ago"). Agent status section: grid of 21 mini agent cards (3 columns on desktop, 1 on mobile). Each shows: agent name, colored status dot (green=active, yellow=idle, red=error, gray=offline), current task or "Idle". Recent activity section: last 10 activity items in a vertical timeline. Use Convex hooks: `useQuery(api.dashboard.getOverviewStats)`, `useQuery(api.dashboard.getAgentStats)`, `useQuery(api.realtime.subscribeToActivity)`. Handle all 5 UI states for each section independently.

### Task D3: Builds Page (Wave 1, 3-4.5h)
Create `app/(dashboard)/builds/page.tsx`. Full build history with filtering and pagination. Layout: filter bar at top (status dropdown, date range, search by build name) + build table + pagination. Build table using shadcn `Table`: columns = Build ID (truncated hash), Name, Status (badge: green=completed, yellow=running, red=failed, gray=pending), Duration, Agent Count, Started At, Actions (view details button). Status badges use shadcn `Badge` with variant colors. Clicking a row navigates to build detail page. Create `app/(dashboard)/builds/[buildId]/page.tsx` — build detail view showing: build metadata, task list (phase by phase), execution timeline (vertical), agent participation chart. Use `useQuery(api.dashboard.listBuilds)` with pagination cursor. Implement client-side filtering (status, search) that updates query params. Handle empty state: "No builds yet — run your first build to see results here" with a CTA button.

### Task D4: Agents Page (Wave 1, 4.5-6h)
Create `app/(dashboard)/agents/page.tsx`. Display all 21 Nova26 agents in a rich card grid. Layout: filter/sort bar (by status, by role, search by name) + agent card grid (3 columns desktop, 2 tablet, 1 mobile). Each agent card (use the `AgentCard` component from `app/components/agent-card.tsx`): agent name + role subtitle, model name (e.g., "Sonnet 4.6"), status indicator (large colored dot + label), stats row: total tasks | success rate | avg duration, last active timestamp, expand button → shows recent task history. Add a "System Overview" section at top: total agents active, total tasks today, system health indicator. Use `useQuery(api.dashboard.getAgentStats)`. Implement sort: by name (A-Z), by activity (most recent first), by success rate (highest first), by task count (most first). Handle loading state with skeleton cards matching the card layout.

### Task D5: Settings Page (Wave 1, 6-7.5h)
Create `app/(dashboard)/settings/page.tsx`. User settings organized in tabs using shadcn `Tabs`. Tab 1 — Profile: name, email (read-only), avatar upload placeholder, timezone selector. Tab 2 — Appearance: theme toggle (light/dark/system) using `next-themes`, sidebar position (left/right), compact mode toggle, font size selector. Tab 3 — API Keys: list of configured API keys (masked, show last 4 chars), add new key form (name + key + provider dropdown), delete key with confirmation dialog. Tab 4 — Notifications: email notifications toggle, in-app notifications toggle, notification frequency (instant/hourly/daily digest). Tab 5 — Billing: current tier display, usage stats (API calls this month, storage used), upgrade CTA for free tier users. Use `useQuery(api.users.getUser)` for current settings, `useMutation(api.users.updateSettings)` for saves. Show toast notification on successful save using shadcn `Toast`.

### Task D6: Real-Time Activity Feed Component (Wave 1, 7.5-8h)
Create the activity feed section for the dashboard overview. This is a real-time updating list powered by Convex subscriptions. Use `useQuery(api.realtime.subscribeToActivity)` — this automatically re-renders when new activity arrives. Display: vertical list of activity items, newest at top. Each item shows: agent avatar (colored circle with first letter), agent name, action description, relative timestamp ("just now", "2m ago", "1h ago"), status badge. Add subtle animation for new items appearing (slide down + fade in, CSS transition). Auto-scroll to top when new items arrive (unless user has scrolled down — detect scroll position). Show "Live" indicator (pulsing green dot) when subscription is active. Show "Reconnecting..." when subscription drops. Max 50 items visible — older items are removed from DOM (not from database). Add "View All Activity" link at bottom → navigates to a dedicated activity page.

### Task D7: Dashboard Polish — Animations + Transitions (Wave 2, 8-10h)
Add micro-interactions and polish across all dashboard pages. Page transitions: fade-in on route change (use Next.js `loading.tsx` with skeleton). Card hover effects: subtle shadow elevation + border color change. Button interactions: scale down slightly on press (active state). Status badge animations: pulse animation on "running" status. Number animations: count-up animation on stat cards when data loads (e.g., "0 → 4,007" over 500ms). Sidebar: smooth collapse/expand animation (width transition 280px → 64px with icon-only mode). Activity feed: staggered entry animation for initial load (each item delays 50ms). Loading skeletons: shimmer animation (gradient sweep left to right). Dark mode transition: smooth color transition (200ms) when toggling theme. Use CSS transitions and `@keyframes` — avoid heavy JS animation libraries. Keep all animations under 300ms for snappy feel. Add `prefers-reduced-motion` media query to disable animations for accessibility.

### Task D8: Dashboard Empty States + CTAs (Wave 2, 10-12h)
Design and implement compelling empty states for every dashboard section. Overview — no builds: illustration placeholder + "Welcome to Nova26" + "Run your first build to see your agents in action" + "Get Started" button. Builds page — no builds: "No builds yet" + brief explanation of what builds are + "Create Build" button. Agents page — no activity: all agents show "Idle" with a message "Your agents are ready. Start a build to see them work." Settings — API keys empty: "No API keys configured" + "Add your first API key to connect external services" + "Add Key" button. Activity feed — no activity: "No recent activity" + "Activity will appear here as your agents work" + subtle animated dots. Each empty state should: use consistent styling (centered, max-width 400px), include an icon or illustration area (use Lucide icons), have a clear CTA button, feel encouraging not empty. Create reusable `EmptyState` component if not already built by Qwen.

### Task D9: Search + Filtering System (Wave 2, 12-14h)
Implement a unified search and filtering system across the dashboard. Create `app/(dashboard)/components/search-bar.tsx` — global search input in the header. Searches across: build names, agent names, activity descriptions. Shows results in a dropdown (shadcn `Command` component). Keyboard shortcut: `Cmd+K` opens search. Builds page filters: status multi-select (completed, running, failed, pending), date range picker (shadcn `Calendar`), duration range (quick filters: <1min, 1-5min, 5-30min, >30min). Agents page filters: status filter (active, idle, error, offline), role filter (dropdown of all 21 roles), model filter (group by provider). All filters sync with URL query params (`?status=running&agent=VENUS`) so filtered views are shareable/bookmarkable. Implement debounced search (300ms delay before querying). Show result count: "Showing 12 of 47 builds". Clear all filters button.

### Task D10: Responsive Design + Mobile Optimization (Wave 3, 16-20h)
Full responsive audit and fix pass across all dashboard pages. Breakpoints: 375px (iPhone SE), 390px (iPhone 14), 768px (iPad), 1024px (laptop), 1440px (desktop). Sidebar: hidden on mobile, sheet overlay on tablet, fixed on desktop. Stat cards: stack vertically on mobile (1 column), 2×2 on tablet, 4×1 on desktop. Agent grid: 1 column mobile, 2 columns tablet, 3 columns desktop. Build table: horizontal scroll on mobile with sticky first column (Build ID). Settings tabs: horizontal scroll on mobile, vertical sidebar on desktop. Activity feed: full width on mobile, right sidebar on desktop overview. Touch targets: minimum 44×44px for all interactive elements. Font sizes: scale down slightly on mobile (base 14px vs 16px desktop). Test with Chrome DevTools device emulation for each breakpoint. Fix any overflow, text truncation, or layout breaking issues.

### Task D11: Accessibility Pass (Wave 3, 20-24h)
Audit all dashboard components for accessibility. Keyboard navigation: every interactive element reachable via Tab, correct tab order, visible focus indicators (2px ring). Screen reader: all images have alt text, all icons have aria-label, all status badges have aria-label (not just color), data tables have proper `<th>` scope attributes. Color contrast: verify all text meets WCAG AA (4.5:1 for normal text, 3:1 for large text) in both light and dark mode. Status indicators: never rely on color alone — always include text label or icon shape. Forms: all inputs have associated labels, error messages linked via `aria-describedby`, required fields marked with `aria-required`. Live regions: activity feed updates announced via `aria-live="polite"`. Reduced motion: all animations respect `prefers-reduced-motion`. Focus management: when modal opens, focus moves to modal; when modal closes, focus returns to trigger. Run axe-core audit on each page, fix all critical and serious issues.

---

## SPRINT: QWEN 3 CODER — "The Component Builder"

**Provider**: OpenRouter (aider)
**Domain**: `app/components/`, `src/workflow/`
**Cannot touch**: `convex/`


### Task Q1: Sidebar Component (Wave 1, 0-1.5h)
Create `app/components/sidebar.tsx`. Fixed sidebar navigation for the dashboard. Structure: logo/brand at top ("Nova26" + icon), navigation links (Dashboard, Builds, Agents, Settings — each with Lucide icon), active state indicator (left border accent + background highlight), collapse toggle button at bottom (chevron icon). Props: `{ collapsed?: boolean, onToggle?: () => void, currentPath: string }`. Navigation items defined as array: `{ label, href, icon, badge? }`. Badge support for notification counts (e.g., "3" on Builds if 3 running). Collapsed mode: only icons visible, tooltip on hover showing label. Smooth width transition: 280px → 64px. Bottom section: user avatar + name (hidden when collapsed), theme toggle button. Mobile: render as shadcn `Sheet` (slide-in overlay from left). Use `usePathname()` from `next/navigation` for active state detection. Keyboard accessible: all links focusable, Enter/Space to navigate. Write component tests: renders all nav items, active state matches current path, collapse toggle works, mobile sheet opens/closes.

### Task Q2: Agent Card Component (Wave 1, 1.5-3h)
Create `app/components/agent-card.tsx`. Rich card displaying a single agent's status and stats. Props: `{ agent: AgentData }` where `AgentData = { id, name, role, model, status, totalTasks, successRate, avgDuration, lastActive, currentTask? }`. Layout: card header (agent name + role badge), status section (large colored indicator + status text), stats row (3 mini stats: tasks | success % | avg time), current task section (if running: task description + progress indicator), footer (last active timestamp + "View Details" link). Status colors: active = green-500, idle = slate-400, error = red-500, offline = slate-300. Status dot should pulse when active (CSS animation). Role badge colors: map each of the 21 roles to a unique color from Tailwind palette. Hover effect: slight elevation + border color change. Click handler: `onClick?: (agentId: string) => void`. Responsive: full width on mobile, fixed 320px on desktop grid. Write 10+ component tests: renders all fields, status colors correct, click handler fires, handles missing optional fields gracefully.

### Task Q3: Build Row Component (Wave 1, 3-4h)
Create `app/components/build-row.tsx`. Table row component for the builds list. Props: `{ build: BuildData, onClick?: (buildId: string) => void }` where `BuildData = { id, name, status, duration, agentCount, startedAt, completedAt?, taskCount, successCount, failCount }`. Layout: row with columns — Build ID (monospace, truncated to 8 chars with tooltip for full ID), Name, Status Badge (shadcn Badge with variant), Duration (formatted: "2m 34s" or "Running..." with elapsed timer for active builds), Agents (count with mini avatar stack), Tasks (progress: "12/15" with mini progress bar), Started (relative time), Actions (kebab menu with View, Re-run, Delete). Active build row: subtle pulsing left border (green). Failed build row: subtle red left border. Hover: row background highlight. Click anywhere on row (except actions menu) triggers `onClick`. Write 8+ tests: renders all columns, status badge variants, duration formatting, click handler, active build animation class applied.

### Task Q4: Activity Item + Activity Feed Components (Wave 1, 4-5.5h)
Create `app/components/activity-item.tsx`. Single activity feed entry. Props: `{ activity: ActivityData }` where `ActivityData = { id, agentName, agentRole, action, details?, status, timestamp, metadata? }`. Layout: horizontal — agent avatar (colored circle with first letter of name, color based on agent role), content column (agent name bold + action text, details in muted text below, relative timestamp), status indicator (right side — small colored dot or icon). Timestamp: "just now", "2m ago", "1h ago", "yesterday", then date. Create `app/components/activity-feed.tsx`. Container for activity items with real-time updates. Props: `{ activities: ActivityData[], isLive?: boolean, maxItems?: number, onLoadMore?: () => void }`. Features: "Live" indicator (pulsing green dot + "Live" text) when `isLive=true`. New item animation: slide down from top with fade-in (CSS transition). Scroll behavior: auto-scroll to newest unless user has scrolled up (track with `useRef` + scroll event listener). "Load More" button at bottom when `onLoadMore` provided. Empty state when no activities. Write 12+ tests: renders items, live indicator, scroll behavior, empty state, load more button.

### Task Q5: Loading Skeleton Components (Wave 1, 5.5-7h)
Create `app/components/loading-skeleton.tsx`. Reusable skeleton components matching the exact layout of each real component. Export: `AgentCardSkeleton` — matches agent card layout (rectangle for header, circles for stats, lines for text). `BuildRowSkeleton` — matches build row (rectangles for each column). `ActivityItemSkeleton` — matches activity item (circle for avatar, lines for text). `StatCardSkeleton` — matches stat card (large number placeholder, small text placeholder). `SidebarSkeleton` — matches sidebar (rectangles for nav items). `PageSkeleton` — full page skeleton combining stat cards + content area. All skeletons use shadcn `Skeleton` component with shimmer animation. Each skeleton accepts `count?: number` prop to render multiple instances (e.g., `<BuildRowSkeleton count={5} />`). Skeletons should be pixel-perfect matches to their real counterparts so the transition from loading → loaded is seamless (no layout shift). Write 6+ tests: each skeleton renders correct number of elements, count prop works.

### Task Q6: Error Boundary + Error States (Wave 1, 7-8h)
Create `app/components/error-boundary.tsx`. React error boundary class component (must be class component for `componentDidCatch`). Props: `{ fallback?: ReactNode, onError?: (error: Error, info: ErrorInfo) => void, onRetry?: () => void }`. Default fallback UI: centered card with warning icon, "Something went wrong" heading, error message (in development only — hidden in production), "Try Again" button that calls `onRetry` or resets error boundary state, "Report Issue" link (placeholder). Create `app/components/error-state.tsx` — functional component for non-boundary error display. Props: `{ title?: string, message?: string, onRetry?: () => void, variant?: "inline" | "fullpage" | "card" }`. Inline variant: small text with retry link (for use inside cards). Card variant: centered in a card with icon. Fullpage variant: centered on screen with large icon. Create `app/components/not-found-state.tsx` — 404-style component for missing resources. "Build not found", "Agent not found" etc. Write 10+ tests: error boundary catches errors, retry resets state, fallback renders, each variant renders correctly.

### Task Q7: Empty State Component (Wave 2, 8-9.5h)
Create `app/components/empty-state.tsx`. Reusable empty state for any section with no data. Props: `{ icon?: LucideIcon, title: string, description?: string, actionLabel?: string, onAction?: () => void, variant?: "default" | "compact" | "illustration" }`. Default variant: centered, max-width 400px, icon (64px, muted color), title (text-lg, font-semibold), description (text-sm, text-muted-foreground), action button (shadcn Button, primary variant). Compact variant: smaller (icon 32px, text-sm), for use inside cards or table bodies. Illustration variant: larger area for a custom illustration/SVG above the text. Pre-built empty states (exported as named components): `EmptyBuilds` — "No builds yet" + "Run your first build to see your agents in action" + rocket icon. `EmptyAgents` — "No agent activity" + "Start a build to see your agents work" + bot icon. `EmptyActivity` — "No recent activity" + "Activity will appear here as agents work" + activity icon. `EmptySearch` — "No results found" + "Try adjusting your search or filters" + search icon. `EmptySettings` — "No API keys configured" + "Add your first API key" + key icon. Write 8+ tests: each variant renders, action button fires, pre-built states render correctly.

### Task Q8: Data Table Component (Wave 2, 9.5-11h)
Create `app/components/data-table.tsx`. Generic, reusable data table built on shadcn `Table`. Props: `{ columns: ColumnDef[], data: T[], isLoading?: boolean, emptyState?: ReactNode, onRowClick?: (row: T) => void, pagination?: PaginationConfig, sorting?: SortConfig }`. `ColumnDef`: `{ key: string, header: string, render?: (value, row) => ReactNode, sortable?: boolean, width?: string, align?: "left" | "center" | "right" }`. Features: sortable columns (click header to toggle asc/desc, show arrow indicator), pagination (page numbers + prev/next, configurable page size 10/20/50), loading state (show `BuildRowSkeleton` rows matching page size), empty state (show provided empty state component or default), row hover highlight, row click handler, responsive (horizontal scroll on mobile with sticky first column). Create `app/components/pagination.tsx` — standalone pagination component. Props: `{ currentPage, totalPages, onPageChange, pageSize, onPageSizeChange }`. Show: "Showing 1-20 of 147 results" + page buttons + page size selector. Write 12+ tests: renders columns, sorting toggles, pagination navigation, loading state, empty state, row click.

### Task Q9: Visual Workflow Engine — Core (Wave 2-3, 11-16h)
Spec: `.nova/specs/grok-r23-eternal-symphony.md` (R23-01 section). Create `src/workflow/workflow-engine.ts`. Implement `WorkflowEngine` class — a persistent visual workflow engine inspired by Temporal/LangGraph patterns. Core concepts: `Workflow` = directed acyclic graph of `Steps`. `Step` = a unit of work assigned to an agent. `Edge` = connection between steps with optional conditions. Implement: `createWorkflow(name: string, steps: StepDefinition[], edges: EdgeDefinition[])` → returns `WorkflowId`. `executeWorkflow(workflowId: WorkflowId, input: unknown)` → starts execution, returns `ExecutionId`. `getExecutionStatus(executionId: ExecutionId)` → returns current state of all steps. `pauseExecution(executionId)`, `resumeExecution(executionId)`, `cancelExecution(executionId)`. Step types: `task` (agent executes work), `decision` (conditional branching based on output), `parallel` (fan-out to multiple steps), `join` (wait for all parallel steps to complete), `human` (pause for human approval). Persistence: serialize workflow state to JSON, restore from JSON (for crash recovery). Create Zod schemas for all types. Write 20+ tests: workflow creation, linear execution, parallel fan-out/join, conditional branching, pause/resume, crash recovery from serialized state.

### Task Q10: Visual Workflow Engine — Visualization Data (Wave 3, 16-20h)
Create `src/workflow/workflow-visualizer.ts`. Generates data structures suitable for rendering workflow graphs in the UI. `toVisualizationData(workflow: Workflow, execution?: Execution)` → returns `{ nodes: VisNode[], edges: VisEdge[], layout: LayoutData }`. `VisNode`: `{ id, label, type, status, position: {x, y}, agent?, duration?, output? }`. `VisEdge`: `{ source, target, label?, condition?, animated? }`. `LayoutData`: `{ width, height, direction: "TB" | "LR" }`. Implement auto-layout algorithm: topological sort → assign layers → minimize edge crossings within layers → compute x/y positions. Nodes colored by status: pending=gray, running=blue (animated border), completed=green, failed=red, skipped=slate. Edges animated when data is flowing (dashed line animation for active edges). Create `src/workflow/workflow-templates.ts` — pre-built workflow templates: `linearBuild` (5 sequential steps: plan → code → test → review → deploy), `parallelAgents` (fan-out to N agents, join results), `reviewPipeline` (code → test → if pass: deploy, if fail: fix → test again). Write 15+ tests: layout algorithm produces valid positions, no overlapping nodes, templates create valid workflows, visualization data matches execution state.

### Task Q11: Component Documentation + Storybook Data (Wave 3, 20-24h)
Create `app/components/README.md` documenting all components: name, props, usage example, variants. Create `app/components/__fixtures__/` directory with mock data for each component: `mock-agents.ts` (21 agents with realistic data), `mock-builds.ts` (10 builds with various statuses), `mock-activities.ts` (50 activity items spanning 24 hours), `mock-workflows.ts` (3 workflow templates with execution data). Each fixture file exports typed constants that can be used in tests and development. Create `app/components/index.ts` — barrel export file for all components. Verify all components: import correctly from barrel file, render without errors with fixture data, pass TypeScript strict checks, have at least basic test coverage. Run `tsc --noEmit` on all component files — 0 errors. Run all component tests — 0 failures.

---

## SPRINT: GLM 5 — "The Polish Worker"

**Provider**: OpenRouter (aider)
**Domain**: `app/(landing)/` (fixes), `src/collaboration/` (Wave 3), mobile responsive
**Cannot touch**: `convex/`

### Task G1: Landing Page CTA → Dashboard Wiring (Wave 1, 0-1.5h)
Update `app/(landing)/components/cta-section.tsx`. Change the primary CTA button from whatever it currently links to → `/dashboard`. If user is authenticated (check with `useConvexAuth()`), button says "Go to Dashboard" and links to `/dashboard`. If not authenticated, button says "Get Started Free" and links to `/sign-up`. Add a secondary CTA: "Watch Demo" (placeholder link to `#demo`). Ensure the CTA section is visually prominent: large button (h-12, px-8), gradient background or accent color, subtle hover animation. Test: click CTA as unauthenticated → lands on sign-up. Click CTA as authenticated → lands on dashboard.

### Task G2: Landing Page Navigation + Auth State (Wave 1, 1.5-3h)
Update the landing page header/navigation component. Add auth-aware navigation items: if authenticated → show "Dashboard" link + user avatar/name + "Sign Out" button. If not authenticated → show "Sign In" and "Sign Up" buttons. Make the header sticky with backdrop blur on scroll (`sticky top-0 z-50 backdrop-blur-sm bg-background/80`). Add mobile hamburger menu (shadcn `Sheet`) with all nav items. Ensure smooth scroll for anchor links (#features, #pricing, etc.) if they exist on the landing page. Add "Skip to main content" link for accessibility (visually hidden, visible on focus). Test navigation flow: landing → sign in → dashboard → sign out → landing.

### Task G3: Landing Page → Dashboard Redirect Logic (Wave 1, 3-4.5h)
Create `app/(landing)/components/auth-redirect.tsx`. Client component that checks auth state on landing page load. If user is authenticated AND navigates to `/` (landing page), show a subtle banner at top: "Welcome back, [name]! Go to your Dashboard →" with a link. Do NOT auto-redirect (user might want to see the landing page). If user navigates to `/sign-in` or `/sign-up` while authenticated, redirect to `/dashboard` automatically. Create `app/(auth)/components/redirect-handler.tsx` — handles post-auth redirects. After successful sign-in: check for `?redirect=` query param → navigate there. If no redirect param → navigate to `/dashboard`. After sign-up: navigate to `/dashboard` with a welcome toast ("Welcome to Nova26!"). Wire redirect handler into auth pages.

### Task G4: Mobile Responsive — Landing Page (Wave 1, 4.5-6h)
Full responsive audit of all landing page components at 375px, 390px, 768px, 1024px, 1440px. Fix: hero section text sizing (scale down heading on mobile, ensure it doesn't overflow), feature cards (stack vertically on mobile, 2-column on tablet, 3-column on desktop), pricing section (stack vertically on mobile), testimonials/social proof (horizontal scroll on mobile if applicable), footer (stack columns vertically on mobile). Ensure all images/illustrations are responsive (`max-w-full h-auto`). Check font sizes: hero heading 3xl on mobile → 5xl on desktop, body text 14px on mobile → 16px on desktop. Verify no horizontal scroll at any breakpoint. Test touch targets: all buttons and links minimum 44×44px tap area. Fix any text truncation issues.

### Task G5: Mobile Responsive — Dashboard Sidebar (Wave 1, 6-8h)
The sidebar is the trickiest responsive element. Implement three modes: Desktop (≥1024px): fixed sidebar, 280px wide, always visible. Tablet (768-1023px): collapsed sidebar, 64px wide (icons only), expand on hover or click. Mobile (<768px): sidebar hidden, accessible via hamburger menu → shadcn `Sheet` overlay from left. Create `app/components/sidebar-mobile.tsx` if needed, or handle within the main sidebar component using a `useMediaQuery` hook. Create `app/hooks/use-media-query.ts` — custom hook that returns current breakpoint. Implement `useSidebarState` hook: manages collapsed/expanded state, persists preference to localStorage, responds to window resize. Ensure sidebar transition is smooth (width animation, opacity for labels). When sidebar is collapsed: show only icons, add tooltips (shadcn `Tooltip`) on hover showing the label. Test at each breakpoint: sidebar renders correctly, transitions smoothly, mobile sheet opens/closes.

### Task G6: Mobile Responsive — Dashboard Pages (Wave 2, 8-10h)
Audit and fix all dashboard pages at mobile breakpoints. Overview page: stat cards stack 1-column on mobile (full width), agent mini-cards stack 2-column on mobile, activity feed full width. Builds page: table becomes card list on mobile (each build as a card instead of table row), filter bar wraps to multiple lines, pagination simplified (prev/next only, no page numbers). Agents page: agent cards 1-column on mobile (full width), filter bar collapses into a "Filters" button that opens a sheet. Settings page: tabs become a dropdown selector on mobile (instead of horizontal tab bar), form inputs full width. For each page, verify: no horizontal overflow, text doesn't truncate unexpectedly, interactive elements have 44px minimum touch targets, spacing is comfortable (not too cramped on small screens).

### Task G7: Mobile Responsive — Typography + Spacing System (Wave 2, 10-12h)
Create a consistent responsive typography and spacing system. Create `app/styles/responsive.css` (or add to `globals.css`). Define responsive font scale: `--text-xs: clamp(0.625rem, 0.6rem + 0.125vw, 0.75rem)`, `--text-sm: clamp(0.75rem, 0.7rem + 0.25vw, 0.875rem)`, `--text-base: clamp(0.875rem, 0.8rem + 0.375vw, 1rem)`, `--text-lg` through `--text-4xl` with similar clamp functions. Define responsive spacing: `--space-page: clamp(1rem, 0.5rem + 2.5vw, 1.5rem)` for page padding, `--space-section: clamp(1.5rem, 1rem + 2.5vw, 3rem)` for section gaps, `--space-card: clamp(0.75rem, 0.5rem + 1.25vw, 1.25rem)` for card padding. Apply these custom properties to Tailwind config or use directly in components. Audit all pages: replace hardcoded `p-6` with responsive padding, replace hardcoded `text-2xl` with responsive text sizes. Verify: text is readable at 375px without zooming, spacing feels proportional at all breakpoints.

### Task G8: Touch Interactions + Gestures (Wave 2, 12-14h)
Optimize all interactive elements for touch devices. Increase tap targets: all buttons minimum `h-11 min-w-[44px]`, all links minimum `py-3 px-4` on mobile. Add touch feedback: active state with slight scale (`active:scale-95`) on buttons and cards. Sidebar mobile: add swipe-to-close gesture on the sheet overlay. Activity feed: add pull-to-refresh gesture (pull down → show refresh indicator → reload data). Build table on mobile (card view): add swipe-left to reveal action buttons (Re-run, Delete). Add haptic feedback hints via CSS (`touch-action: manipulation` to remove 300ms tap delay). Ensure no double-tap-to-zoom issues (set `<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">`). Test on actual mobile device or Chrome DevTools touch simulation.

### Task G9: CRDT Collaboration — Core Library (Wave 3, 14-18h)
Spec: `.nova/specs/grok-r24-immortal-omniverse.md` (R24-03 section). Create `src/collaboration/crdt-engine.ts`. Implement a CRDT (Conflict-free Replicated Data Type) engine for real-time multi-user collaboration. Core types: `CRDTDocument` — a collaborative document that multiple users can edit simultaneously. `Operation` — an atomic change (insert, delete, update) with vector clock timestamp. `VectorClock` — tracks causal ordering across multiple clients. Implement LWW-Register (Last Writer Wins Register) for simple key-value fields. Implement G-Counter (Grow-only Counter) for metrics that only increase. Implement OR-Set (Observed-Remove Set) for collections where items can be added and removed. Each CRDT type: `merge(local, remote)` → deterministic merge with no conflicts, `apply(operation)` → apply a local change, `serialize()` / `deserialize()` → JSON-safe representation. Create `src/collaboration/crdt-engine.test.ts` with 25+ tests: commutativity (merge(A,B) === merge(B,A)), associativity, idempotency, concurrent edits resolve deterministically, serialization round-trip.

### Task G10: CRDT Collaboration — Sync Protocol (Wave 3, 18-22h)
Create `src/collaboration/sync-protocol.ts`. Implement the synchronization protocol for CRDT documents across clients. `SyncManager` class: `connect(documentId: string)` → establish sync session, `disconnect()` → clean up. `broadcastOperation(op: Operation)` → send local change to all peers. `receiveOperation(op: Operation)` → merge remote change into local state. `getState()` → current merged document state. `getPendingOps()` → operations not yet acknowledged by server. Implement optimistic local application: apply operation locally immediately, send to server, if server rejects → rollback and re-apply server state. Implement conflict resolution: vector clock comparison determines causal ordering, concurrent operations merged via CRDT rules (no conflicts by design). Implement presence: `updatePresence(cursor: Position, selection?: Range)` → broadcast cursor position, `getPresences()` → all connected users' cursor positions. Create `src/collaboration/sync-protocol.test.ts` with 20+ tests: two clients making concurrent edits converge to same state, presence updates propagate, disconnect/reconnect preserves state, network partition recovery.

### Task G11: Final Responsive QA Pass (Wave 3, 22-24h)
Complete end-to-end responsive testing across all pages and breakpoints. Create a QA checklist and test each item: Landing page renders correctly at 375px, 768px, 1024px, 1440px. Dashboard overview: all sections visible and properly laid out at each breakpoint. Builds page: table/card view switches correctly, filters work on mobile. Agents page: card grid responsive, filters accessible on mobile. Settings page: tabs/dropdown work, forms usable on mobile. Activity feed: scrollable, new items animate, live indicator visible. Sidebar: three modes work correctly (desktop/tablet/mobile). Auth pages: forms centered, inputs full width on mobile, keyboard doesn't obscure inputs. Navigation: all links work, active states correct, mobile menu opens/closes. Fix any remaining issues found during QA. Run `tsc --noEmit` — 0 errors. Document any known responsive issues in `.nova/RESPONSIVE-QA.md`.

---

## SPRINT: MISTRAL LARGE — "The Fixer"

**Provider**: OpenRouter (aider)
**Domain**: ALL files (read + fix only — cannot create new feature files)
**Role**: Dedicated QA — review, fix, validate


### Task M1: Initial TypeScript Audit (Wave 1, 2-3h)
Wait for first output from Sonnet, Haiku, DeepSeek, and Qwen (hours 0-2). Then pull all changes and run `npx tsc --noEmit`. Catalog every TypeScript error: file path, line number, error code, error message. Group errors by: missing imports, type mismatches, missing type declarations, `any` usage, strict mode violations. Fix ALL errors in priority order: 1) missing imports (usually just wrong path or missing `.js` extension), 2) type mismatches (add proper types or fix function signatures), 3) missing declarations (create `.d.ts` files if needed), 4) `any` usage (replace with proper types). After fixing, run `tsc --noEmit` again — must be 0 errors. Document: how many errors found, how many fixed, which files were modified, any errors that need the original author to fix (design issues, not typos). Report findings.

### Task M2: Test Suite Validation — Round 1 (Wave 1, 3-4.5h)
Run `vitest run` across the entire test suite. Catalog every failing test: file path, test name, error message, expected vs actual. Categorize failures: 1) import errors (wrong paths after refactoring), 2) mock issues (mocks don't match updated interfaces), 3) logic errors (test expectations wrong due to implementation changes), 4) missing dependencies (test needs a module that hasn't been created yet). Fix categories 1 and 2 yourself. For category 3: determine if the test or the implementation is wrong — fix whichever is incorrect. For category 4: skip the test with a `TODO` comment explaining what's needed. After fixing, run `vitest run` again — target 0 failures (some skipped is OK). Report: total tests, passing, failing, skipped, and what you fixed.

### Task M3: ESM Import Audit (Wave 1, 4.5-6h)
The project uses ESM with `.js` extensions on imports. Audit ALL new files created by other workers. Check: every relative import has `.js` extension (e.g., `import { foo } from './bar.js'` not `import { foo } from './bar'`). Check: no CommonJS patterns (`require()`, `module.exports`). Check: `package.json` has `"type": "module"`. Check: `tsconfig.json` has `"module": "ESNext"` or `"NodeNext"`. Fix any violations. Common mistake: workers forget `.js` extensions on new files. Run a grep: `grep -rn "from '\.\/" src/ --include="*.ts" | grep -v "\.js'" | grep -v "\.test\." | grep -v node_modules` to find missing extensions. Fix all findings. Verify: `tsc --noEmit` still passes after fixes.

### Task M4: Convex Function Validation (Wave 1, 6-8h)
Audit all Convex functions created by Haiku 4. Check each function in `convex/dashboard.ts`, `convex/auth.ts`, `convex/realtime.ts`, `convex/users.ts`. Verify: every query/mutation uses proper validators (`v.string()`, `v.number()`, `v.id()`, etc. — no `v.any()`). Verify: every function checks auth via `ctx.auth.getUserIdentity()`. Verify: every query uses `.withIndex()` for indexed lookups (no full table scans). Verify: return types are explicit and match what the frontend expects. Verify: error handling exists (try/catch on mutations, proper error messages). Cross-reference with `convex/schema.ts`: do the queries reference tables and indexes that actually exist? Fix any issues found. Run `npx convex dev` (or dry-run equivalent) to verify functions compile.

### Task M5: Frontend Component Validation (Wave 2, 8-10h)
Audit all React components created by DeepSeek and Qwen. Check each component for: proper TypeScript types on all props (no `any`), proper handling of all 5 UI states (Loading, Empty, Error, Partial, Populated), correct Convex hook usage (`useQuery` returns `undefined` while loading — is this handled?), proper key props on all `.map()` rendered lists, no inline styles (Tailwind only), proper accessibility attributes (aria-labels on icons, alt text on images, role attributes where needed). Check for common React 19 issues: are client components marked with `"use client"` directive? Are server components accidentally importing client-only hooks? Fix all issues found. Run `tsc --noEmit` after fixes.

### Task M6: Integration Point Validation (Wave 2, 10-12h)
Verify all integration points between workers' code. Check: Dashboard pages import Convex API correctly (`import { api } from "@convex/_generated/api"`). Check: Convex function names in `useQuery(api.dashboard.listBuilds)` match actual exported function names. Check: TypeScript types between frontend and Convex are compatible (Convex `Doc<"builds">` type matches what components expect). Check: Auth flow works end-to-end — providers.tsx wraps correctly, auth guard checks correctly, sign-in/sign-up forms call correct Convex auth functions. Check: Bridge (`src/convex/bridge.ts`) calls correct Convex mutations with correct argument shapes. Fix any mismatches. This is the most critical task — integration bugs are the #1 cause of "it works in isolation but breaks together."

### Task M7: Security Audit (Wave 2, 12-14h)
Security review of all new code. Check auth: no Convex functions accessible without authentication (except public landing page data). Check: no API keys or secrets hardcoded in source files (should all be in env vars). Check: no `dangerouslySetInnerHTML` without sanitization. Check: all user input is validated before use (Convex validators on mutations, Zod schemas on bridge). Check: no SQL injection equivalent in Convex queries (Convex is safe by design, but verify no string interpolation in query filters). Check: CORS headers are restrictive (only allow the app's domain). Check: rate limiting exists on auth endpoints. Check: error messages don't leak internal details (stack traces, file paths) in production. Fix any security issues found. Document findings in `.nova/SECURITY-AUDIT.md`.

### Task M8: Performance Review (Wave 2, 14-16h)
Performance audit of all new code. Check: no N+1 query patterns in Convex (e.g., fetching a list then querying each item individually — should use batch or join). Check: React components don't re-render unnecessarily (are expensive computations wrapped in `useMemo`? Are callbacks wrapped in `useCallback`?). Check: no large objects in React state that could cause slow renders. Check: images are optimized (Next.js `<Image>` component, not raw `<img>`). Check: no synchronous heavy computation on the main thread. Check: Convex queries use pagination for large result sets. Check: bundle size — are there unnecessary large dependencies imported? Fix any performance issues found. Run Lighthouse on the dashboard (if possible) and report scores.

### Task M9: Test Suite Validation — Round 2 (Wave 3, 16-18h)
Second full test run after Wave 2 code is complete. Run `vitest run` — catalog all new failures. Many will be from Wave 2 features (workflow engine, model routing, CRDT). Fix what you can: import errors, mock issues, type mismatches. For complex logic failures: determine if test or implementation is wrong, fix the correct one. Run `tsc --noEmit` — fix any new TypeScript errors from Wave 2 code. Verify: all Wave 1 tests still pass (no regressions). Report: total test count (should be significantly higher than Wave 1), pass rate, any persistent failures that need original author attention.

### Task M10: Final QA Pass — Zero Errors (Wave 3, 18-22h)
The final quality gate before production. Run `npx tsc --noEmit` — must be exactly 0 errors. Run `vitest run` — must be 0 failures (skipped tests OK if documented). Check: no `console.log` in production code (search with `grep -rn "console.log" src/ app/ convex/ --include="*.ts" --include="*.tsx" | grep -v ".test." | grep -v node_modules`). Check: no `TODO` or `FIXME` comments that indicate incomplete work (document them if they exist). Check: no unused imports (TypeScript should catch these with `noUnusedLocals`). Check: no unused variables. Check: all files have consistent formatting (run prettier if configured). Fix everything. This is the last line of defense before deployment.

### Task M11: Cross-Worker Dependency Verification (Wave 3, 22-24h)
Final verification that all workers' code works together. Trace the critical path: 1) Landing page loads → 2) Click CTA → 3) Sign-up page renders → 4) Create account → 5) Redirect to dashboard → 6) Overview page loads with data → 7) Navigate to builds → 8) Navigate to agents → 9) Navigate to settings → 10) Sign out → 11) Back to landing. For each step, verify the code path: which files are involved, do imports resolve, do types match, do Convex queries return expected shapes. Create a dependency map: `LANDING (GLM) → AUTH (Sonnet) → DASHBOARD (DeepSeek) → CONVEX (Haiku) → BRIDGE (Sonnet) → ENGINE (Kimi)`. Verify no broken links in the chain. Report: final status of all integration points, any remaining issues, confidence level for production deployment.

---

## SPRINT: LLAMA 4 MAVERICK — "Wave 3 Parallel"

**Provider**: OpenRouter (aider)
**Domain**: `src/observability/`, `src/memory/`, `src/model-db/`
**Cannot touch**: `convex/`, `app/`

### Task L1: Eval Framework — Core Types + Registry (Wave 1, 0-2h)
Spec: `.nova/specs/grok-r23-eternal-symphony.md` (R23-05 section). Create `src/observability/types.ts`. Define core types: `EvalCase` = `{ id, input, expectedOutput?, metadata? }`. `EvalResult` = `{ caseId, actualOutput, score: number, latency: number, tokenCount: number, pass: boolean, details?: string }`. `EvalSuite` = `{ id, name, cases: EvalCase[], scoringFn: ScoringFunction, threshold: number }`. `ScoringFunction` = `(expected: unknown, actual: unknown) => number` (0-1 scale). `EvalRun` = `{ suiteId, results: EvalResult[], startedAt, completedAt, summary: EvalSummary }`. `EvalSummary` = `{ totalCases, passed, failed, avgScore, avgLatency, p50Latency, p95Latency, totalTokens }`. Create `src/observability/eval-registry.ts`. Implement `EvalRegistry` class: `registerSuite(suite: EvalSuite)`, `getSuite(id: string)`, `listSuites()`, `removeSuite(id: string)`. Suites stored in-memory with optional JSON persistence. Validate all inputs with Zod schemas. Write 15+ tests: type validation, registry CRUD, duplicate ID rejection, listing and filtering.

### Task L2: Eval Framework — Scoring Functions (Wave 1, 2-4h)
Create `src/observability/scoring.ts`. Implement built-in scoring functions: `exactMatch(expected, actual)` → 1.0 if identical, 0.0 otherwise. `fuzzyMatch(expected, actual, threshold?)` → Levenshtein distance normalized to 0-1 scale. `containsMatch(expected, actual)` → 1.0 if actual contains expected, 0.0 otherwise. `jsonMatch(expected, actual)` → deep equality check, partial score for partially matching objects (count matching keys / total keys). `semanticSimilarity(expected, actual)` → cosine similarity of simple TF-IDF vectors (no external API needed). `codeMatch(expected, actual)` → normalize whitespace and compare (for code generation eval). `compositeScore(scorers: ScoringFunction[], weights: number[])` → weighted average of multiple scorers. Each scorer: handles null/undefined gracefully (returns 0), handles type mismatches (returns 0 with warning), is deterministic (same inputs → same output). Create `src/observability/scoring.test.ts` with 20+ tests: each scorer with various inputs, edge cases (empty strings, null, mismatched types), composite scoring with different weights.

### Task L3: Eval Framework — Runner (Wave 1, 4-6h)
Create `src/observability/eval-runner.ts`. Implement `EvalRunner` class — executes eval suites against a target function. `run(suiteId: string, targetFn: TargetFunction, options?: RunOptions)` → executes all cases, returns `EvalRun`. `TargetFunction` = `(input: unknown) => Promise<unknown>`. `RunOptions` = `{ concurrency?: number, timeout?: number, retryOnFailure?: boolean, maxRetries?: number }`. Execution: run cases with configurable concurrency (default 5 parallel). Per-case: call targetFn with input, measure latency, apply scoring function, determine pass/fail against threshold. Handle: target function timeout (mark as failed with "timeout" detail), target function error (mark as failed with error message), scoring function error (mark as failed with "scoring error" detail). After all cases: compute summary statistics (avg/p50/p95 latency, pass rate, total tokens). Implement progress callback: `onProgress?: (completed: number, total: number, latest: EvalResult) => void`. Write 15+ tests: successful run, partial failures, timeout handling, concurrency limiting, progress callback, summary statistics accuracy.

### Task L4: Eval Framework — Golden Set Testing (Wave 1, 6-8h)
Create `src/observability/golden-sets.ts`. Implement golden set management — curated test cases that represent expected behavior. `GoldenSet` extends `EvalSuite` with: `version: number`, `createdAt: Date`, `updatedAt: Date`, `approvedBy?: string`. `createGoldenSet(name: string, cases: EvalCase[])` → creates versioned golden set. `updateGoldenSet(id: string, changes: GoldenSetUpdate)` → increments version, tracks what changed. `compareToGolden(suiteId: string, run: EvalRun)` → compare current results against golden set baseline, return `{ regressions: EvalCase[], improvements: EvalCase[], unchanged: EvalCase[] }`. `promoteRunToGolden(runId: string)` → take a successful run's results and make them the new golden baseline. Implement regression detection: if a case that previously passed now fails, flag as regression. If a case that previously failed now passes, flag as improvement. Generate regression report: `{ totalRegressions, regressionDetails, suggestedAction: "investigate" | "update-golden" | "rollback" }`. Write 15+ tests: golden set CRUD, version tracking, regression detection, improvement detection, promotion flow.

### Task L5: Eval Framework — Persistence + History (Wave 2, 8-10h)
Create `src/observability/eval-store.ts`. Implement persistent storage for eval runs and golden sets. `EvalStore` class with JSON file-based storage (simple, no external DB dependency). `saveRun(run: EvalRun)` → write to `data/eval-runs/{suiteId}/{runId}.json`. `getRun(runId: string)` → read from file. `listRuns(suiteId: string, options?: { limit?, offset?, since? })` → list runs for a suite, sorted by date desc. `saveGoldenSet(set: GoldenSet)` → write to `data/golden-sets/{setId}.json`. `getGoldenSet(id: string)` → read from file. `getRunHistory(suiteId: string, limit: number)` → return last N runs with summary stats for trend analysis. Implement `EvalTrend` analysis: `analyzeTrend(suiteId: string, windowSize: number)` → return `{ direction: "improving" | "degrading" | "stable", avgScoreChange, latencyChange, passRateChange }` based on last N runs. Write 15+ tests: save/load round-trip, listing with pagination, trend analysis accuracy, file corruption handling (graceful error on malformed JSON).

### Task L6: Eval Framework — Reporting (Wave 2, 10-12h)
Create `src/observability/eval-reporter.ts`. Generate human-readable and machine-readable reports from eval runs. `generateReport(run: EvalRun, options?: ReportOptions)` → returns `EvalReport`. `ReportOptions` = `{ format: "markdown" | "json" | "html", includeDetails?: boolean, compareToGolden?: string }`. Markdown report format: header with suite name + run date, summary table (total/pass/fail/score/latency), per-case results table (case ID, input preview, expected, actual, score, pass/fail), regression section (if comparing to golden), recommendations section. JSON report: structured data suitable for dashboards. Implement `generateDiffReport(runA: EvalRun, runB: EvalRun)` → side-by-side comparison of two runs, highlighting improvements and regressions. Implement `generateTrendReport(suiteId: string, runs: number)` → trend over last N runs with sparkline-compatible data (array of scores over time). Write 10+ tests: report generation for each format, diff report accuracy, trend report data correctness.

### Task L7: Infinite Memory — Core Store (Wave 2, 12-15h)
Spec: `.nova/specs/grok-r23-eternal-symphony.md` (R23-03 section). Create `src/memory/memory-store.ts`. Implement hierarchical memory system inspired by Mem0/Letta patterns. Memory hierarchy: L1 (Working Memory) — current task context, fast access, limited size (max 100 items). L2 (Session Memory) — current session context, medium access, larger (max 1000 items). L3 (Long-term Memory) — persistent across sessions, indexed for retrieval, unlimited. `MemoryItem` = `{ id, content: string, embedding?: number[], tags: string[], importance: number (0-1), createdAt, accessedAt, accessCount, level: "L1" | "L2" | "L3" }`. Implement `MemoryStore` class: `store(content: string, tags: string[], importance?: number)` → creates item in L1. `retrieve(query: string, options?: { level?, limit?, minImportance? })` → search across levels, return ranked results. `promote(itemId: string)` → move item to higher importance / lower level number. `demote(itemId: string)` → move to higher level number or archive. `compress(level: "L1" | "L2")` → summarize and merge similar items to free space. Implement LRU eviction within each level when capacity exceeded — least recently accessed items demoted to next level. Write 20+ tests: store/retrieve, promotion/demotion, LRU eviction, compression, cross-level search.

### Task L8: Infinite Memory — Retrieval + Ranking (Wave 2-3, 15-18h)
Create `src/memory/memory-retriever.ts`. Implement intelligent retrieval across the memory hierarchy. `retrieve(query: string, options: RetrievalOptions)` → returns ranked `MemoryItem[]`. Retrieval strategies: `keyword` — simple text matching (TF-IDF scoring). `semantic` — cosine similarity on embeddings (if available). `temporal` — prefer recently accessed items. `importance` — prefer high-importance items. `hybrid` — weighted combination of all strategies. `RetrievalOptions` = `{ strategy, limit, minScore, levels?, tags?, timeRange? }`. Implement `RelevanceScorer`: combines multiple signals into a single relevance score. Signals: text similarity (0-1), recency decay (exponential decay based on time since last access), importance weight, access frequency bonus, tag match bonus. Implement `MemoryIndex` — inverted index for fast keyword lookup. `addToIndex(item: MemoryItem)`, `removeFromIndex(itemId: string)`, `search(terms: string[])` → returns item IDs with TF-IDF scores. Write 20+ tests: each retrieval strategy returns correct results, hybrid ranking produces sensible ordering, index updates correctly on add/remove, recency decay works as expected.

### Task L9: Infinite Memory — Compression + Summarization (Wave 3, 18-20h)
Create `src/memory/memory-compressor.ts`. Implement memory compression to keep the store manageable over time. `compress(items: MemoryItem[])` → returns fewer items that capture the same information. Compression strategies: `merge` — find items with >80% text overlap, merge into single item with combined tags and max importance. `summarize` — group items by tag, create a summary item for each group (concatenate content, extract key phrases). `prune` — remove items below importance threshold that haven't been accessed in N days. `archive` — move items to cold storage (separate file) instead of deleting. Implement `CompressionScheduler`: automatically compress L1 when >80% full, L2 when >90% full. Track compression history: `{ timestamp, level, itemsBefore, itemsAfter, strategy, bytesFreed }`. Implement `decompressArchive(query: string)` — search archived items and restore relevant ones to L3. Write 15+ tests: merge correctly combines similar items, summarize produces valid summaries, prune respects importance threshold, archive/restore round-trip, scheduler triggers at correct thresholds.

### Task L10: AI Model Database — Registry (Wave 3, 20-22h)
Spec: `.nova/specs/grok-r24-immortal-omniverse.md` (R24-01 section). Create `src/model-db/model-registry.ts`. Implement a comprehensive AI model database. `ModelEntry` = `{ id, name, provider, family, version, capabilities: Capability[], pricing: PricingInfo, performance: PerformanceMetrics, limits: ModelLimits, status: "active" | "deprecated" | "preview", updatedAt }`. `Capability` = `{ name: string, score: number (0-1) }` — e.g., "code-generation": 0.95, "reasoning": 0.8. `PricingInfo` = `{ inputPerMToken, outputPerMToken, currency: "USD" }`. `PerformanceMetrics` = `{ latencyP50, latencyP95, throughputTokensPerSec, contextWindow, maxOutputTokens }`. `ModelLimits` = `{ rateLimit: { requestsPerMinute, tokensPerMinute }, maxConcurrent }`. Implement `ModelRegistry` class: `register(entry: ModelEntry)`, `get(id: string)`, `list(filters?: ModelFilter)`, `update(id: string, updates: Partial<ModelEntry>)`, `deprecate(id: string)`. `ModelFilter` = `{ provider?, capability?, minScore?, maxPrice?, status? }`. Pre-populate with entries for: Ollama models (local), Anthropic (Sonnet, Haiku, Opus), OpenRouter models (DeepSeek, Qwen, Mistral, Llama, GLM). Write 15+ tests: CRUD operations, filtering, deprecation, pre-populated entries correct.

### Task L11: AI Model Database — Capability Matching + Cost Optimization (Wave 3, 22-24h)
Create `src/model-db/capability-matcher.ts`. Implement intelligent model selection based on task requirements. `findBestModel(requirements: TaskRequirements, constraints: SelectionConstraints)` → returns ranked `ModelEntry[]`. `TaskRequirements` = `{ capabilities: Map<string, number>, minContextWindow?, preferLocal?: boolean }`. `SelectionConstraints` = `{ maxCostPerRequest?, maxLatency?, providers?: string[], excludeModels?: string[] }`. Matching algorithm: 1) Filter models that meet all hard constraints (context window, provider, not excluded). 2) Score each model: capability match score (weighted sum of min(model_score, required_score) / required_score), cost efficiency score (inverse of price, normalized), latency score (inverse of p50 latency, normalized), locality bonus (if preferLocal and model is Ollama). 3) Rank by composite score. Create `src/model-db/cost-calculator.ts`: `estimateCost(model: ModelEntry, inputTokens: number, outputTokens: number)` → estimated cost in USD. `compareCosts(models: ModelEntry[], tokenEstimate: { input, output })` → cost comparison table. `findCheapestModel(requirements: TaskRequirements)` → cheapest model that meets requirements. Wire into the model router (Kimi's `src/llm/model-router.ts`) — model-db provides the data, router makes the decisions. Write 15+ tests: capability matching ranks correctly, cost calculation accurate, constraints filter properly, cheapest model selection, integration with router interface.

---
